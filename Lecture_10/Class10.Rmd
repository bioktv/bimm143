---
title: "Class10"
author: "Kimberly"
date: "2/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Unsupervised Learning Mini Project
- https://bioboot.github.io/bimm143_W20/class-material/lab-9-bimm143.html
```{r Creating the variables}
fna.data <- read.csv("WisconsinCancer.csv")
wisc.df <- data.frame(fna.data)

wisc.df
```
Notice: Last column "X" has a bunch of NA's. 
- There was probably some mistake from the Excel file, so we should clean it up to work with what we want to work with.
- (i.e., no "X" (col 33), no "ID" (col 1), no "Diagnosis" (col 2)).

```{r Cleaning up}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[, 3:32])
# Can use head(wisc.data) to check beginning and ends of dataset to check if you are doing it correctly

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
```

*Q1. How many patients are in this dataset?*
```{r Q1: Patients #}
nrow(wisc.data)
```
*Q2. How many M's (cancer) and how many B's (non-cancer) are in that "diagnosis" column?*
```{r Q2: Count of M and B}
table(wisc.df[,2])
# OR table(wisc.df$diagnosis)
```
*Q3. How many variables/features in the data are suffixed with "_mean"?*
```{r Q3: "_mean" suffix}
# looking at all the column names within wisc.data
# you could probably use this to eyeball it but would be tedious for big datasets
colnames(wisc.data)

# finds a patten that you give it within the input vector given
grep("_mean", colnames(wisc.data))

# value = T tells you WHAT the matches are
# value = F tells you WHERE the matches are
grep("_mean", colnames(wisc.data), value = T)

# look at the length of counts with "_mean"
length(grep("_mean", colnames(wisc.data), value = T))
```

# Principal Component Analysis
- Before we do PCA, we should check if we need to re-scale our data.
- Recall 2 common reasons for scaling data include:

1. The input variables use different units of measurement.
- The input variables have significantly different variances.
- Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. 

Use the `colMeans()` and `apply()` functions like you’ve done before.
```{r Check if we need to rescale}
# Check column means and standard deviations
colMeans(wisc.data)

# 2 is the columns, 1 is for rows
# sd is the standard deviation function
# gives output in scientific notation if numbers are large/tiny
apply(wisc.data,2,sd)

# rounds the values to 2 decimal points
round(apply(wisc.data,2,sd), 2)
```
**Numbers are all over the place, so it looks like we need to set scale = TRUE.**

```{r Start PCA}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```
*Q4: From your results, what proportion of the original variance is captured by the first principal components (PC1)?*
**A4: "PC1 captures 44.27% of original variance within the whole dataset."**

"PC1 AND PC2 capture 63% of variance ("Cumulative Prop") within the whole dataset."
*Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?*
**A5. 72.6% in the first 3 PCs.**

## Interpret PCA results
```{r Plotting PC1 and PC2}
# A MESS. TOO MANY VARIABLES LAYERING ON TOP OF EACH OTHER
biplot(wisc.pr)

# We need to do our own PC1 vs. PC2 plot and color by the diagnosis
attributes(wisc.pr)

plot(wisc.pr$x[,1], wisc.pr$x[,2], 
     col = wisc.df$diagnosis,
     xlab = "PC1", ylab = "PC2")
abline(h = 0, col = "darkgreen", lty = 2)
abline(v = 0, col = "darkgreen", lty = 2)
```
- The benign black cells are relatively similar and grouped together neatly.
- The malignant red cells are more spread out, implying that there are many ways to be cancerous.

```{r Plotting PC1 and PC3}
plot(wisc.pr$x[,1], wisc.pr$x[,3], 
     col = wisc.df$diagnosis,
     xlab = "PC1", ylab = "PC3")
```

## Cluser in PC spaces
First, let's see if we can cluster the original data.

### Hierarchical clustering of case data
```{r Create}
# scale the data b/c it is a mess otherwise
data.scaled <- scale(wisc.data)

# calculate the Euclidean distances
data.dist <- dist(data.scaled)

# create the hcluster
wisc.hclust <- hclust(data.dist)

plot(wisc.hclust)
```

#### Combining methods
```{r Cluster using results of PCA data rather than raw data}
# Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". 
# We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis.
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:3]), method = "ward.D2")

plot(wisc.pr.hclust)

```

```{r Cut the tree}
grps_of_3 <- cutree(wisc.pr.hclust, k = 3)
table(grps_of_3)

```
"There are 111 in group 1, 92 in group 2, 366 in group 3."

```{r Plot}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps_of_3)
```

We can use `table()` to compare `$diagnosis` vector with our cluster results vector.
```{r}
table(grps_of_3, wisc.df$diagnosis)
```
"In cluster 1, there are 0 benign and 111 malignant."
"In cluster 2, there are 24 benign and 68 malignant."
"In cluster 3, there are 333 benign and 33 malignant."
- implying that cluster 3 is mainly benign. 333 true positives. (BUT there are 33 false positives. #misclassification)
# sensitivity vs. specificity # dependent on the system

## Predictions w/ `predict()`
```{r}
new <- read.csv("new_samples.csv")
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r Add the new set to my plot}
plot(wisc.pr$x[,1], wisc.pr$x[,2],
     col = wisc.df$diagnosis)
points(npc[, 1], npc[, 2], col = "blue", pch = 15, cex = 3)
text(npc[,1], npc[,2], labels = c(1,2), col = "white")
```

You should be checking on patient 2 because they seem to land on the malignant area.
